{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe84f30",
   "metadata": {},
   "source": [
    "# Sanity Check\n",
    "ben gave me sample files which are indicative of requests that would be sent to DeepInfra. Sanity checking that I'm getting reasonable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748d890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished api-req3.json -> 200 (20.63s)\n",
      "Finished api-req2.json -> 200 (25.95s)\n",
      "Finished 5.json -> 200 (32.48s)\n",
      "Finished api-req1.json -> 200 (42.134s)\n",
      "Finished 2.json -> 200 (47.835s)\n",
      "\n",
      "=== Cost / Token Usage Data ===\n",
      "            file  status  prompt_tokens  completion_tokens  total_tokens  \\\n",
      "0  api-req3.json     200           4805                731          5536   \n",
      "1  api-req2.json     200           4805                986          5791   \n",
      "2         5.json     200           3164               1307          4471   \n",
      "3  api-req1.json     200           5603               1694          7297   \n",
      "4         2.json     200           5192               1949          7141   \n",
      "\n",
      "   estimated_cost  latency_sec  \\\n",
      "0        0.000313       20.630   \n",
      "1        0.000339       25.950   \n",
      "2        0.000289       32.480   \n",
      "3        0.000450       42.134   \n",
      "4        0.000455       47.835   \n",
      "\n",
      "                                    response_preview  \n",
      "0  **1. 테이블 구조 분석:**\\n\\n*   **테이블 제목:** “(표 1) OI...  \n",
      "1  **1. 테이블 구조 분석:**\\n\\n*   **테이블 제목:** 표2 소비자물가 ...  \n",
      "2  {\"figure_title\":\"Characteristics of InternVL 1...  \n",
      "3  Okay, here are the customized instructions for...  \n",
      "4  Okay, here are the custom instructions for con...  \n"
     ]
    }
   ],
   "source": [
    "import os, json, requests, time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "\n",
    "REQUESTS_DIR = \"sample_requests\"\n",
    "FORCED_MODEL = \"google/gemma-3-12b-it\"\n",
    "MAX_WORKERS = 8  # number of threads\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "def send_request(fname: str):\n",
    "    \"\"\"Send a single request and return usage/cost info.\"\"\"\n",
    "    path = os.path.join(REQUESTS_DIR, fname)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    payload[\"model\"] = FORCED_MODEL  # override model\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            \"https://api.deepinfra.com/v1/openai/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=120,\n",
    "        )\n",
    "        latency = time.perf_counter() - start\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        usage = data.get(\"usage\", {})\n",
    "        content = \"\"\n",
    "        if \"choices\" in data and data[\"choices\"]:\n",
    "            content = data[\"choices\"][0][\"message\"].get(\"content\", \"\")[:200]\n",
    "\n",
    "        return {\n",
    "            \"file\": fname,\n",
    "            \"status\": resp.status_code,\n",
    "            \"prompt_tokens\": usage.get(\"prompt_tokens\"),\n",
    "            \"completion_tokens\": usage.get(\"completion_tokens\"),\n",
    "            \"total_tokens\": usage.get(\"total_tokens\"),\n",
    "            \"estimated_cost\": usage.get(\"estimated_cost\"),\n",
    "            \"latency_sec\": round(latency, 3),\n",
    "            \"response_preview\": content,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"file\": fname,\n",
    "            \"status\": \"error\",\n",
    "            \"prompt_tokens\": None,\n",
    "            \"completion_tokens\": None,\n",
    "            \"total_tokens\": None,\n",
    "            \"estimated_cost\": None,\n",
    "            \"latency_sec\": None,\n",
    "            \"response_preview\": f\"❌ {e}\",\n",
    "        }\n",
    "\n",
    "# Collect all JSON files\n",
    "json_files = [f for f in os.listdir(REQUESTS_DIR) if f.endswith(\".json\")]\n",
    "\n",
    "records = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(send_request, fname): fname for fname in json_files}\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        records.append(result)\n",
    "        print(f\"Finished {result['file']} -> {result['status']} ({result['latency_sec']}s)\")\n",
    "\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "print(\"\\n=== Cost / Token Usage Data ===\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44baa64",
   "metadata": {},
   "source": [
    "# Cost and throughput\n",
    "based on `parameter_analysis.ipynb`, a typical claim might have around 4,658 pages. at around 1.37 chunks per page, that's 6,381 chunks. Each chunk takes 2-5 of these API calls to produce, with some miscilaneous surrounding API calls. For our sake, we'll say each chunk requires 4 API calls, or 25,524 VLM calls per claim.\n",
    "\n",
    "The following estimates costs and bandwidth based on how many claims might be processed simultaniously, and how quickly they should be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurable\n",
    "PARALLEL_CLAIMS = 20 # how many claims we want to be able to process in parallel\n",
    "PROCESSING_TIME_SECONDS = 600 #600 = want each claim to be done in 10 minutes\n",
    "\n",
    "# constants\n",
    "VLM_CALLS_PER_CLAIM = 25524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f00fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_cost = float(df.estimated_cost.mean())\n",
    "average_latency = float(df.latency_sec.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53d5183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Throughput & Cost Estimates ===\n",
      "Average cost per call: $0.000369\n",
      "Average latency per call: 33.806s\n",
      "Calls needed per second: 850.80\n",
      "Effective calls/sec per worker: 0.03\n",
      "Required parallel workers: 28762.0\n",
      "Cost per claim: $9.42\n",
      "Cost per 10.0 minutes: $188.38\n",
      "Cost per hour: $1,130.29\n",
      "Cost per day: $27,127.07\n"
     ]
    }
   ],
   "source": [
    "# Calls per second needed to finish all claims in time\n",
    "calls_needed_per_second = (PARALLEL_CLAIMS * VLM_CALLS_PER_CLAIM) / PROCESSING_TIME_SECONDS\n",
    "\n",
    "# Throughput per worker (calls/sec)\n",
    "calls_per_worker_per_second = 1 / average_latency\n",
    "\n",
    "# How many parallel workers required\n",
    "required_parallelism = calls_needed_per_second / calls_per_worker_per_second\n",
    "\n",
    "# Cost per claim\n",
    "cost_per_claim = average_cost * VLM_CALLS_PER_CLAIM\n",
    "\n",
    "# Total cost per batch (parallel claims in the given window)\n",
    "total_cost = cost_per_claim * PARALLEL_CLAIMS\n",
    "\n",
    "# Scale up to hourly and daily\n",
    "cost_per_hour = total_cost * (3600 / PROCESSING_TIME_SECONDS)\n",
    "cost_per_day = cost_per_hour * 24\n",
    "\n",
    "print(\"=== Throughput & Cost Estimates ===\")\n",
    "print(f\"Average cost per call: ${average_cost:.6f}\")\n",
    "print(f\"Average latency per call: {average_latency:.3f}s\")\n",
    "print(f\"Calls needed per second: {calls_needed_per_second:.2f}\")\n",
    "print(f\"Effective calls/sec per worker: {calls_per_worker_per_second:.2f}\")\n",
    "print(f\"Required parallel workers: {required_parallelism:.1f}\")\n",
    "print(f\"Cost per claim: ${cost_per_claim:,.2f}\")\n",
    "print(f\"Cost per {PROCESSING_TIME_SECONDS/60:.1f} minutes: ${total_cost:,.2f}\")\n",
    "print(f\"Cost per hour: ${cost_per_hour:,.2f}\")\n",
    "print(f\"Cost per day: ${cost_per_day:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea080db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepinfrathroughputtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
