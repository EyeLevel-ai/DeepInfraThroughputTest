{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe84f30",
   "metadata": {},
   "source": [
    "# Sanity Check\n",
    "ben gave me sample files which are indicative of requests that would be sent to DeepInfra. Sanity checking that I'm getting reasonable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748d890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 5.json -> 200 (18.58s)\n",
      "Finished api-req3.json -> 200 (23.429s)\n",
      "Finished api-req2.json -> 200 (23.895s)\n",
      "Finished api-req1.json -> 200 (26.745s)\n",
      "Finished 2.json -> 200 (39.793s)\n",
      "\n",
      "=== Cost / Token Usage Data ===\n",
      "            file  status  prompt_tokens  completion_tokens  total_tokens  \\\n",
      "0         5.json     200           3164                927          4091   \n",
      "1  api-req3.json     200           4805               1224          6029   \n",
      "2  api-req2.json     200           4805               1198          6003   \n",
      "3  api-req1.json     200           5603               1352          6955   \n",
      "4         2.json     200           5192               2340          7532   \n",
      "\n",
      "   estimated_cost  latency_sec  \\\n",
      "0        0.000251       18.580   \n",
      "1        0.000363       23.429   \n",
      "2        0.000360       23.895   \n",
      "3        0.000415       26.745   \n",
      "4        0.000494       39.793   \n",
      "\n",
      "                                    response_preview  \n",
      "0  {\"figure_title\":\"Characteristics of InternVL 1...  \n",
      "1  **1. 테이블 구조 분석:**\\n\\n*   **테이블 제목:** “(표 1) OI...  \n",
      "2  1. **분석 테이블 구조**: 테이블은 “표2”라는 제목과 “국소 소비자물가 증감...  \n",
      "3  Okay, here are customized instructions for con...  \n",
      "4  Okay, here are the customized instructions for...  \n"
     ]
    }
   ],
   "source": [
    "import os, json, requests, time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "\n",
    "REQUESTS_DIR = \"sample_requests\"\n",
    "FORCED_MODEL = \"google/gemma-3-12b-it\"\n",
    "MAX_WORKERS = 8  # number of threads\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "def send_request(fname: str):\n",
    "    \"\"\"Send a single request and return usage/cost info.\"\"\"\n",
    "    path = os.path.join(REQUESTS_DIR, fname)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    payload[\"model\"] = FORCED_MODEL  # override model\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            \"https://api.deepinfra.com/v1/openai/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=120,\n",
    "        )\n",
    "        latency = time.perf_counter() - start\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        usage = data.get(\"usage\", {})\n",
    "        content = \"\"\n",
    "        if \"choices\" in data and data[\"choices\"]:\n",
    "            content = data[\"choices\"][0][\"message\"].get(\"content\", \"\")[:200]\n",
    "\n",
    "        return {\n",
    "            \"file\": fname,\n",
    "            \"status\": resp.status_code,\n",
    "            \"prompt_tokens\": usage.get(\"prompt_tokens\"),\n",
    "            \"completion_tokens\": usage.get(\"completion_tokens\"),\n",
    "            \"total_tokens\": usage.get(\"total_tokens\"),\n",
    "            \"estimated_cost\": usage.get(\"estimated_cost\"),\n",
    "            \"latency_sec\": round(latency, 3),\n",
    "            \"response_preview\": content,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"file\": fname,\n",
    "            \"status\": \"error\",\n",
    "            \"prompt_tokens\": None,\n",
    "            \"completion_tokens\": None,\n",
    "            \"total_tokens\": None,\n",
    "            \"estimated_cost\": None,\n",
    "            \"latency_sec\": None,\n",
    "            \"response_preview\": f\"❌ {e}\",\n",
    "        }\n",
    "\n",
    "# Collect all JSON files\n",
    "json_files = [f for f in os.listdir(REQUESTS_DIR) if f.endswith(\".json\")]\n",
    "\n",
    "records = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(send_request, fname): fname for fname in json_files}\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        records.append(result)\n",
    "        print(f\"Finished {result['file']} -> {result['status']} ({result['latency_sec']}s)\")\n",
    "\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "print(\"\\n=== Cost / Token Usage Data ===\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44baa64",
   "metadata": {},
   "source": [
    "# Cost and throughput\n",
    "based on `parameter_analysis.ipynb`, a typical claim might have around 4,658 pages. at around 1.37 chunks per page, that's 6,381 chunks. Each chunk takes 2-5 of these API calls to produce, with some miscilaneous surrounding API calls. For our sake, we'll say each chunk requires 4 API calls, or 25,524 VLM calls per claim.\n",
    "\n",
    "The following estimates costs and bandwidth based on how many claims might be processed simultaniously, and how quickly they should be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbd2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurable\n",
    "PARALLEL_CLAIMS = 20 # how many claims we want to be able to process in parallel\n",
    "PROCESSING_TIME_SECONDS = 600 #600 = want each claim to be done in 10 minutes\n",
    "\n",
    "# constants\n",
    "VLM_CALLS_PER_CLAIM = 25524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f00fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_cost = float(df.estimated_cost.mean())\n",
    "average_latency = float(df.latency_sec.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a53d5183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Throughput & Cost Estimates ===\n",
      "Average cost per call: $0.000377\n",
      "Average latency per call: 26.488s\n",
      "Calls needed per second: 850.80\n",
      "Effective calls/sec per worker: 0.04\n",
      "Required parallel workers: 22536.3\n",
      "Cost per claim: $9.61\n",
      "Cost per 10.0 minutes: $192.20\n",
      "Cost per hour: $1,153.20\n",
      "Cost per day: $27,676.92\n"
     ]
    }
   ],
   "source": [
    "# Calls per second needed to finish all claims in time\n",
    "calls_needed_per_second = (PARALLEL_CLAIMS * VLM_CALLS_PER_CLAIM) / PROCESSING_TIME_SECONDS\n",
    "\n",
    "# Throughput per worker (calls/sec)\n",
    "calls_per_worker_per_second = 1 / average_latency\n",
    "\n",
    "# How many parallel workers required\n",
    "required_parallelism = calls_needed_per_second / calls_per_worker_per_second\n",
    "\n",
    "# Cost per claim\n",
    "cost_per_claim = average_cost * VLM_CALLS_PER_CLAIM\n",
    "\n",
    "# Total cost per batch (parallel claims in the given window)\n",
    "total_cost = cost_per_claim * PARALLEL_CLAIMS\n",
    "\n",
    "# Scale up to hourly and daily\n",
    "cost_per_hour = total_cost * (3600 / PROCESSING_TIME_SECONDS)\n",
    "cost_per_day = cost_per_hour * 24\n",
    "\n",
    "print(\"=== Throughput & Cost Estimates ===\")\n",
    "print(f\"Average cost per call: ${average_cost:.6f}\")\n",
    "print(f\"Average latency per call: {average_latency:.3f}s\")\n",
    "print(f\"Calls needed per second: {calls_needed_per_second:.2f}\")\n",
    "print(f\"Effective calls/sec per worker: {calls_per_worker_per_second:.2f}\")\n",
    "print(f\"Required parallel workers: {required_parallelism:.1f}\")\n",
    "print(f\"Cost per claim: ${cost_per_claim:,.2f}\")\n",
    "print(f\"Cost per {PROCESSING_TIME_SECONDS/60:.1f} minutes: ${total_cost:,.2f}\")\n",
    "print(f\"Cost per hour: ${cost_per_hour:,.2f}\")\n",
    "print(f\"Cost per day: ${cost_per_day:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6383c",
   "metadata": {},
   "source": [
    "# Comparing Requests\n",
    "curious how much of it is system prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75030636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Sending full request (default max_tokens)\n",
      "✅ Done (200) in 26.876s\n",
      "\n",
      "================================================================================\n",
      "Sending full request (max_tokens=1)\n",
      "✅ Done (200) in 2.659s\n",
      "\n",
      "================================================================================\n",
      "Sending subset request (messages=[0, 2], max_tokens=1)\n",
      "✅ Done (200) in 0.507s\n",
      "\n",
      "\n",
      "=== Cost / Token Usage Data ===\n",
      "                 label  status  prompt_tokens  completion_tokens  total_tokens  estimated_cost  latency_sec\n",
      "          FULL_DEFAULT     200           4805               1079          5884        0.000332       26.876\n",
      "         FULL_MAXTOK_1     200           4805                  1          4806        0.000192        2.659\n",
      "SUBSET_[0, 2]_MAXTOK_1     200           2948                  1          2949        0.000118        0.507\n"
     ]
    }
   ],
   "source": [
    "import os, json, requests, time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# =========================================\n",
    "# Config\n",
    "# =========================================\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "\n",
    "# JSON_PATH = \"sample_requests/api-req1.json\"\n",
    "# MESSAGE_INDEXES = [0, 2]\n",
    "\n",
    "# JSON_PATH = \"sample_requests/api-req2.json\"\n",
    "# MESSAGE_INDEXES = [0, 2]\n",
    "\n",
    "JSON_PATH = \"sample_requests/api-req3.json\"\n",
    "MESSAGE_INDEXES = [0, 2]\n",
    "\n",
    "\n",
    "FORCED_MODEL = \"google/gemma-3-12b-it\"\n",
    "DEEPINFRA_URL = \"https://api.deepinfra.com/v1/openai/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Helpers\n",
    "# =========================================\n",
    "def send_request(label: str, payload: dict):\n",
    "    \"\"\"Send a single DeepInfra request and return usage/cost info.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            DEEPINFRA_URL, headers=headers, json=payload, timeout=120\n",
    "        )\n",
    "        latency = round(time.perf_counter() - start, 3)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        usage = data.get(\"usage\", {})\n",
    "        content_preview = \"\"\n",
    "        if \"choices\" in data and data[\"choices\"]:\n",
    "            msg = data[\"choices\"][0].get(\"message\", {})\n",
    "            content_preview = msg.get(\"content\", \"\")[:200]\n",
    "\n",
    "        return {\n",
    "            \"label\": label,\n",
    "            \"status\": resp.status_code,\n",
    "            \"prompt_tokens\": usage.get(\"prompt_tokens\"),\n",
    "            \"completion_tokens\": usage.get(\"completion_tokens\"),\n",
    "            \"total_tokens\": usage.get(\"total_tokens\"),\n",
    "            \"estimated_cost\": usage.get(\"estimated_cost\"),\n",
    "            \"latency_sec\": latency,\n",
    "            # \"response_preview\": content_preview,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"label\": label,\n",
    "            \"status\": \"error\",\n",
    "            \"prompt_tokens\": None,\n",
    "            \"completion_tokens\": None,\n",
    "            \"total_tokens\": None,\n",
    "            \"estimated_cost\": None,\n",
    "            \"latency_sec\": None,\n",
    "            # \"response_preview\": f\"❌ {e}\",\n",
    "        }\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Main\n",
    "# =========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load base JSON\n",
    "    with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        base_payload = json.load(f)\n",
    "\n",
    "    base_payload[\"model\"] = FORCED_MODEL\n",
    "\n",
    "    # ---- 1. Full Request (default max_tokens) ----\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Sending full request (default max_tokens)\")\n",
    "    result_full_default = send_request(\"FULL_DEFAULT\", base_payload)\n",
    "    print(f\"✅ Done ({result_full_default['status']}) in {result_full_default['latency_sec']}s\\n\")\n",
    "\n",
    "    # ---- 2. Full Request (max_tokens = 1) ----\n",
    "    full_short = base_payload.copy()\n",
    "    full_short[\"max_tokens\"] = 1\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Sending full request (max_tokens=1)\")\n",
    "    result_full_1tok = send_request(\"FULL_MAXTOK_1\", full_short)\n",
    "    print(f\"✅ Done ({result_full_1tok['status']}) in {result_full_1tok['latency_sec']}s\\n\")\n",
    "\n",
    "    # ---- 3. Subset Request (max_tokens = 1) ----\n",
    "    subset_payload = base_payload.copy()\n",
    "    subset_payload[\"messages\"] = [\n",
    "        base_payload[\"messages\"][i]\n",
    "        for i in MESSAGE_INDEXES\n",
    "        if 0 <= i < len(base_payload[\"messages\"])\n",
    "    ]\n",
    "    subset_payload[\"max_tokens\"] = 1\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Sending subset request (messages={MESSAGE_INDEXES}, max_tokens=1)\")\n",
    "    result_subset_1tok = send_request(f\"SUBSET_{MESSAGE_INDEXES}_MAXTOK_1\", subset_payload)\n",
    "    print(f\"✅ Done ({result_subset_1tok['status']}) in {result_subset_1tok['latency_sec']}s\\n\")\n",
    "\n",
    "    # ---- Combine results ----\n",
    "    df = pd.DataFrame([result_full_default, result_full_1tok, result_subset_1tok])\n",
    "    print(\"\\n=== Cost / Token Usage Data ===\")\n",
    "    print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea080db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecae3802",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cac451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepinfrathroughputtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
